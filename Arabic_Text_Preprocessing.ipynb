{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arabic Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-docx\n",
    "!pip install pyarabic\n",
    "!pip install qalsadi\n",
    "!pip install camel-tools\n",
    "!pip install camel-tools --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import qalsadi.lemmatizer as ql\n",
    "from camel_tools.morphology.database import MorphologyDB\n",
    "from camel_tools.morphology.analyzer import Analyzer\n",
    "from camel_tools.sentiment import SentimentAnalyzer\n",
    "import os\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mount Google Drive and Set Up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive.mount('/gdrive')\n",
    "os.environ['CAMELTOOLS_DATA'] = '/gdrive/MyDrive/camel_tools'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read .docx File Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_docx_file(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    full_text = []\n",
    "    for paragraph in doc.paragraphs:\n",
    "        full_text.append(paragraph.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "docx_file_path = '/path/to/transcipt.docx'\n",
    "arabic_text = read_docx_file(docx_file_path)\n",
    "df = pd.DataFrame([arabic_text], columns=['arabic_text'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arabic and English Punctuation Removal Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arab_Punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
    "english_Punctuations = string.punctuation\n",
    "listOfPunctuations = arab_Punctuations + english_Punctuations\n",
    "\n",
    "def Remove_Punctuations_ArEng(text):\n",
    "    translator = str.maketrans('', '', listOfPunctuations)\n",
    "    text = text.translate(translator)\n",
    "    return text\n",
    "\n",
    "df[\"arabic_text\"] = df[\"arabic_text\"].apply(Remove_Punctuations_ArEng)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove English Text and Numbers Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Remove_EnglishText(text):\n",
    "    engTEXT = r'[a-zA-Z0-9]+'\n",
    "    text = re.sub(engTEXT, '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "df[\"arabic_text\"] = df[\"arabic_text\"].apply(Remove_EnglishText)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Arabic Stopwords Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "def Remove_ArabicStopWords(text):\n",
    "    stopWords = stopwords.words('arabic')\n",
    "    text = [word for word in text if word not in stopWords]\n",
    "    return text\n",
    "\n",
    "df[\"arabic_text\"] = df[\"arabic_text\"].apply(Remove_ArabicStopWords)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arabic Text Normalization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalize_Arabic(text):\n",
    "    text = re.sub(r'[إأآا]', 'ا', text)\n",
    "    return text\n",
    "\n",
    "df[\"arabic_text\"] = df[\"arabic_text\"].apply(Normalize_Arabic)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arabic Text Tokenization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "def tokenize_arabic(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "df[\"tokenized_text\"] = df[\"arabic_text\"].apply(tokenize_arabic)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect All Words and Create Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for text in df[\"tokenized_text\"]:\n",
    "    for word in text:\n",
    "        if word.isalnum():\n",
    "            all_words.append(word)\n",
    "\n",
    "print(all_words)\n",
    "\n",
    "from collections import Counter\n",
    "word_freq = Counter(all_words)\n",
    "word_freq_df = pd.DataFrame(word_freq.items(), columns=['Word', 'Frequency'])\n",
    "word_freq_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis Using CAMeL Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa = SentimentAnalyzer.pretrained()\n",
    "sentences = sent_tokenize(\" \".join([str(text) for text in df[\"arabic_text\"]]))\n",
    "sentiments = [sa.predict(sentence) for sentence in sentences]\n",
    "sentiment_scores = [1 if s == 'positive' else -1 if s == 'negative' else 0 for s in sentiments]\n",
    "sentiment_df = pd.DataFrame(list(zip(sentences, sentiments, sentiment_scores)), columns=['Sentence', 'Sentiment', 'Score'])\n",
    "print(SentimentAnalyzer.pretrained_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling Using LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='arabic')\n",
    "doc_term_matrix = vectorizer.fit_transform(sentences)\n",
    "LDA = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "LDA.fit(doc_term_matrix)\n",
    "topics = LDA.transform(doc_term_matrix).argmax(axis=1)\n",
    "topic_df = pd.DataFrame(list(zip(sentences, topics)), columns=['Sentence', 'Topic'])\n",
    "topic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition Using CAMeL Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel_tools.ner import NERecognizer\n",
    "\n",
    "ner = NERecognizer.pretrained()\n",
    "entities = ner.predict(\" \".join(sentences))\n",
    "entity_list = [(e['text'], e['type']) for e in entities]\n",
    "entity_df = pd.DataFrame(entity_list, columns=['Entity', 'Type'])\n",
    "entity_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_df.to_csv('word_frequency.csv', index=False)\n",
    "sentiment_df.to_csv('sentiments.csv', index=False)\n",
    "topic_df.to_csv('topics.csv', index=False)\n",
    "entity_df.to_csv('entities.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
